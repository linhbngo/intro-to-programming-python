{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-means clustering algorithm represents each cluster by its corresponding cluster centroid. The algorithm partition the input data into *k* disjoint clusters by iteratively applying the following two steps:\n",
    "\n",
    "1. Form *k* clusters by assigning each instance to its nearest centroid.\n",
    "2. Recompute the centroid of each cluster.\n",
    "\n",
    "In this section, we perform k-means clustering on a toy example of movie ratings dataset. We first create the dataset as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = [['john',5,5,2,1],['mary',4,5,3,2],['bob',4,4,4,3],['lisa',2,2,4,5],['lee',1,2,3,4],['harry',2,1,5,5]]\n",
    "titles = ['user','Jaws','Star Wars','Exorcist','Omen']\n",
    "movies = pd.DataFrame(ratings,columns=titles)\n",
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example dataset, the first 3 users liked action movies (Jaws and Star Wars) while the last 3 users enjoyed horror movies (Exorcist and Omen). Our goal is to apply k-means clustering on the users to identify groups of users with similar movie preferences.\n",
    "\n",
    "The example below shows how to apply k-means clustering (with k=2) on the movie ratings data. We must remove the \"user\" column first before applying the clustering algorithm. The cluster assignment for each user is displayed as a dataframe object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "\n",
    "data = movies.drop('user',axis=1)\n",
    "k_means = cluster.KMeans(n_clusters=2, max_iter=50, random_state=1)\n",
    "k_means.fit(data) \n",
    "labels = k_means.labels_\n",
    "pd.DataFrame(labels, index=movies.user, columns=['Cluster ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-means clustering algorithm assigns the first three users to one cluster and the last three users to the second cluster. The results are consistent with our expectation. We can also display the centroid for each of the two clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = k_means.cluster_centers_\n",
    "pd.DataFrame(centroids,columns=data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that cluster 0 has higher ratings for the horror movies whereas cluster 1 has higher ratings for action movies. The cluster centroids can be applied to other users to determine their cluster assignments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "testData = np.array([[4,5,1,2],[3,2,4,4],[2,3,4,1],[3,2,3,3],[5,4,1,4]])\n",
    "labels = k_means.predict(testData)\n",
    "labels = labels.reshape(-1,1)\n",
    "usernames = np.array(['paul','kim','liz','tom','bill']).reshape(-1,1)\n",
    "cols = movies.columns.tolist()\n",
    "cols.append('Cluster ID')\n",
    "newusers = pd.DataFrame(np.concatenate((usernames, testData, labels), axis=1),columns=cols)\n",
    "newusers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the number of clusters in the data, we can apply k-means with varying number of clusters from 1 to 6 and compute their corresponding sum-of-squared errors (SSE) as shown in the example below. The \"elbow\" in the plot of SSE versus number of clusters can be used to estimate the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "numClusters = [1,2,3,4,5,6]\n",
    "SSE = []\n",
    "for k in numClusters:\n",
    "    k_means = cluster.KMeans(n_clusters=k)\n",
    "    k_means.fit(data)\n",
    "    SSE.append(k_means.inertia_)\n",
    "\n",
    "plt.plot(numClusters, SSE)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('SSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Complete the codes below to perform a K-mean clustering on the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.metrics as sm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/iris.csv', header=None)\n",
    "df.columns = ['sepal length', 'sepal width', 'petal length', 'petal width', 'class']\n",
    "print (df.head())\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('class', axis=1)  \n",
    "y_text = df['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris dataset has three classes: Iris-setosa, Iris-virginica, and Iris-versicolor. The class labels need to be converted to numeric form for Kmean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.Series(range(0,y_text.size), index = range(0, y_text.size))\n",
    "for i in range(0,y_text.size):\n",
    "    if (y_text[i] == '______'):\n",
    "        y[i] = 0;\n",
    "    elif (y_text[i] == ______):\n",
    "        y[i] = 1;\n",
    "    else:\n",
    "        y[i] = 2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=__)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    " \n",
    "# Create a colormap (how many colors?)\n",
    "colormap = np.random.rand(__,)\n",
    " \n",
    "# Plot the Original Classifications\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X['petal length'], X['petal width'], c=colormap[y], s=40)\n",
    "plt.title('Real Classification')\n",
    "\n",
    "predY = np.choose(model.labels_, [2, 0, 1]).astype(np.int64)\n",
    "\n",
    "# Plot the Models Classifications\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X['petal length'], X['petal width'], c=colormap[predY], s=40)\n",
    "plt.title('K Mean Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we examine examples of applying hierarchical clustering to the vertebrate dataset used in Module 6 (Classification). Specifically, we illustrate the results of using 3 hierarchical clustering algorithms provided by the Python scipy library: (1) single link (MIN), (2) complete link (MAX), and (3) group average. Other hierarchical clustering algorithms provided by the library include centroid-based and Ward's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/vertebrate.csv',header='infer')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "names = data['Name']\n",
    "Y = data['Class']\n",
    "X = data.drop(['Name','Class'],axis=1)\n",
    "Z = hierarchy.linkage(X.as_matrix(), 'single')\n",
    "dn = hierarchy.dendrogram(Z,labels=names.tolist(),orientation='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = hierarchy.linkage(X.as_matrix(), 'complete')\n",
    "dn = hierarchy.dendrogram(Z,labels=names.tolist(),orientation='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = hierarchy.linkage(X.as_matrix(), 'average')\n",
    "dn = hierarchy.dendrogram(Z,labels=names.tolist(),orientation='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In density-based clustering, we identify the individual clusters as high-density regions that are separated by regions of low density. DBScan is one of the most popular density based clustering algorithms. In DBScan, data points are classified into 3 types---core points, border points, and noise points---based on the density of their local neighborhood. The local neighborhood density is defined according to 2 parameters:  radius of neighborhood size (eps) and minimum number of points in the neighborhood (min_samples). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/chameleon.data', delimiter=' ', names=['x','y'])\n",
    "data.plot.scatter(x='x',y='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the DBScan clustering algorithm on the data by setting the neighborhood radius (eps) to 15.5 and minimum number of points (min_samples) to be 5. The clusters are assigned to IDs between 0 to 8 while the noise points are assigned to a cluster ID equals to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "db = DBSCAN(eps=15.5, min_samples=5).fit(data)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = pd.DataFrame(db.labels_,columns=['Cluster ID'])\n",
    "result = pd.concat((data,labels), axis=1)\n",
    "result.plot.scatter(x='x',y='y',c='Cluster ID', colormap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main limitations of the k-means clustering algorithm is its tendency to seek for globular-shaped clusters. Thus, it does not work when applied to datasets with arbitrary-shaped clusters or when the cluster centroids overlapped with one another. Spectral clustering can overcome this limitation by exploiting properties of the similarity graph to overcome such limitations. To illustrate this, consider the following two-dimensional datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data1 = pd.read_csv('data/2d_data.txt', delimiter=' ', names=['x','y'])\n",
    "data2 = pd.read_csv('data/elliptical.txt', delimiter=' ', names=['x','y'])\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n",
    "data1.plot.scatter(x='x',y='y',ax=ax1)\n",
    "data2.plot.scatter(x='x',y='y',ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we demonstrate the results of applying k-means to the datasets (with k=2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "\n",
    "k_means = cluster.KMeans(n_clusters=2, max_iter=50, random_state=1)\n",
    "k_means.fit(data1)\n",
    "labels1 = pd.DataFrame(k_means.labels_,columns=['Cluster ID'])\n",
    "result1 = pd.concat((data1,labels1), axis=1)\n",
    "\n",
    "k_means2 = cluster.KMeans(n_clusters=2, max_iter=50, random_state=1)\n",
    "k_means2.fit(data2)\n",
    "labels2 = pd.DataFrame(k_means2.labels_,columns=['Cluster ID'])\n",
    "result2 = pd.concat((data2,labels2), axis=1)\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n",
    "result1.plot.scatter(x='x',y='y',c='Cluster ID',colormap='jet',ax=ax1)\n",
    "ax1.set_title('K-means Clustering')\n",
    "result2.plot.scatter(x='x',y='y',c='Cluster ID',colormap='jet',ax=ax2)\n",
    "ax2.set_title('K-means Clustering')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above show the poor performance of k-means clustering. Next, we apply spectral clustering to the datasets. Spectral clustering converts the data into a similarity graph and applies the normalized cut graph partitioning algorithm to generate the clusters. In the example below, we use the Gaussian radial basis function as our affinity (similarity) measure. Users need to tune the kernel parameter (gamma) value in order to obtain the appropriate clusters for the given dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "import pandas as pd\n",
    "\n",
    "spectral = cluster.SpectralClustering(n_clusters=2,random_state=1,affinity='rbf',gamma=5000)\n",
    "spectral.fit(data1)\n",
    "labels1 = pd.DataFrame(spectral.labels_,columns=['Cluster ID'])\n",
    "result1 = pd.concat((data1,labels1), axis=1)\n",
    "\n",
    "spectral2 = cluster.SpectralClustering(n_clusters=2,random_state=1,affinity='rbf',gamma=100)\n",
    "spectral2.fit(data2)\n",
    "labels2 = pd.DataFrame(spectral2.labels_,columns=['Cluster ID'])\n",
    "result2 = pd.concat((data2,labels2), axis=1)\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n",
    "result1.plot.scatter(x='x',y='y',c='Cluster ID',colormap='jet',ax=ax1)\n",
    "ax1.set_title('Spectral Clustering')\n",
    "result2.plot.scatter(x='x',y='y',c='Cluster ID',colormap='jet',ax=ax2)\n",
    "ax2.set_title('Spectral Clustering')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "This is a free-form exercise. You are to use the *titanic3.xls* data and perform a K-mean clustering analysis on the data. In data mining, and in data science in general, we always want to *let the data do the talking*. Use one or more of the techniques above to analyze this data, and draw some conclusions from your analysis. Think about a story you want to tell based on the Titanic data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
