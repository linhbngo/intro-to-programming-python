{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [scikit-learn](https://scikit-learn.org/stable/modules/naive_bayes.html) library support a number of Naive Bayes classifiers: \n",
    "\n",
    "- Gaussian: It assumes that continuous features follow a normal distribution.\n",
    "- Multinomial: It is useful if your features are discrete.\n",
    "- Bernoulli: The binomial model is useful if your features are binary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titanic Survival Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "\n",
    "# Importing dataset\n",
    "data = pd.read_csv(\"data/titanic_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variable to numeric\n",
    "data[\"Sex_cleaned\"]=np.where(data[\"Sex\"]==\"male\",0,1)\n",
    "data[\"Embarked_cleaned\"]=np.where(data[\"Embarked\"]==\"S\",0,\n",
    "                                  np.where(data[\"Embarked\"]==\"C\",1,\n",
    "                                           np.where(data[\"Embarked\"]==\"Q\",2,3)\n",
    "                                          )\n",
    "                                 )\n",
    "# Cleaning dataset of NaN\n",
    "data=data[[\n",
    "    \"Survived\",\n",
    "    \"Pclass\",\n",
    "    \"Sex_cleaned\",\n",
    "    \"Age\",\n",
    "    \"SibSp\",\n",
    "    \"Parch\",\n",
    "    \"Fare\",\n",
    "    \"Embarked_cleaned\"\n",
    "]].dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset in training and test datasets\n",
    "X_train, X_test = train_test_split(data, test_size=0.5, random_state=int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classifier\n",
    "gnb = GaussianNB()\n",
    "used_features =[\n",
    "    \"Pclass\",\n",
    "    \"Sex_cleaned\",\n",
    "    \"Age\",\n",
    "    \"SibSp\",\n",
    "    \"Parch\",\n",
    "    \"Fare\",\n",
    "    \"Embarked_cleaned\"\n",
    "]\n",
    "\n",
    "# Train classifier\n",
    "gnb.fit(\n",
    "    X_train[used_features].values,\n",
    "    X_train[\"Survived\"]\n",
    ")\n",
    "y_pred = gnb.predict(X_test[used_features])\n",
    "\n",
    "# Print results\n",
    "print(\"Number of mislabeled points out of a total {} points : {}, performance {:05.2f}%\"\n",
    "      .format(\n",
    "          X_test.shape[0],\n",
    "          (X_test[\"Survived\"] != y_pred).sum(),\n",
    "          100*(1-(X_test[\"Survived\"] != y_pred).sum()/X_test.shape[0])\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "used_features =[\"Fare\"]\n",
    "y_pred = gnb.fit(X_train[used_features].values, X_train[\"Survived\"]).predict(X_test[used_features])\n",
    "print(\"Number of mislabeled points out of a total {} points : {}, performance {:05.2f}%\"\n",
    "      .format(\n",
    "          X_test.shape[0],\n",
    "          (X_test[\"Survived\"] != y_pred).sum(),\n",
    "          100*(1-(X_test[\"Survived\"] != y_pred).sum()/X_test.shape[0])\n",
    "))\n",
    "print(\"Std Fare not_survived {:05.2f}\".format(np.sqrt(gnb.sigma_)[0][0]))\n",
    "print(\"Std Fare survived: {:05.2f}\".format(np.sqrt(gnb.sigma_)[1][0]))\n",
    "print(\"Mean Fare not_survived {:05.2f}\".format(gnb.theta_[0][0]))\n",
    "print(\"Mean Fare survived: {:05.2f}\".format(gnb.theta_[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam SMS\n",
    "\n",
    "In this example, we will attempt to apply Naive Bayes on textual data. The target data set is [a collection of spam sms](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). \n",
    "\n",
    "The data table contains two columns separated by a tab. The two columns are **label** and **message**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_table('data/SMSSpamCollection',  \n",
    "                   sep='\\t', \n",
    "                   header=None,\n",
    "                   names=['label', 'message'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike numerical data, textual data requires additional processing. First, we need to convert the categorical labels into numerical values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.label.map({'ham': 0, 'spam': 1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we need to convert all characters in the message to lower case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['message'] = df.message.map(lambda x: x.lower())  \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, remove any punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['message'] = df.message.str.replace('[^\\w\\s]', '')  \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth, tokenize the messages into into single words using nltk. First, we have to import and download the tokenizer from the console. \n",
    "\n",
    "To work with textual data, we will use the **nltk** library. NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, and wrappers for industrial-strength NLP (natural language processing) libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['message'] = df['message'].apply(nltk.word_tokenize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fifth, we will perform some word stemming. The idea of stemming is to normalize our text for all variations of words carry the same meaning, regardless of the tense. One of the most popular stemming algorithms is the Porter Stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "df['message'] = df['message'].apply(lambda x: [stemmer.stem(y) for y in x])  \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will transform the data into occurrences, which will be the features that we will feed into our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# This converts the list of words into space-separated strings\n",
    "df['message'] = df['message'].apply(lambda x: ' '.join(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()  \n",
    "counts = count_vect.fit_transform(df['message'])  \n",
    "print(len(count_vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could leave it as the simple word-count per message, but it is better to use Term Frequency Inverse Document Frequency, more known as tf-idf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer().fit(counts)\n",
    "counts = transformer.transform(counts)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "Now that we have performed feature extraction from our data, it is time to build our model. We will start by splitting our data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(counts, df['label'], \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=69)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, all that we have to do is initialize the Naive Bayes Classifier and fit the data. For text classification problems, the Multinomial Naive Bayes Classifier is well-suited:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB().fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluating the Model\n",
    "Once we have put together our classifier, we can evaluate its performance in the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "print(np.mean(predicted == y_test))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Our simple Naive Bayes Classifier has 98.2% accuracy with this specific test set! But it is not enough by just providing the accuracy, since our dataset is imbalanced when it comes to the labels (86.6% legitimate in contrast to 13.4% spam). It could happen that our classifier is over-fitting the legitimate class while ignoring the spam class. To solve this uncertainty, let's have a look at the confusion matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "\n",
    "Complete the following code that apply Naive Bayes Classifier to a sentiment text data set. There are three datasets inside the sentiment_label_sentences for IMDB, Yelp, and Amazone review. You can select one to perform the classification process on. \n",
    "\n",
    "Hint, while this question is similar to the procedure of the Spam example, the structure of the data file is different. You need to review the data from the Spam example, and then identify the difference such that you can fill in the right contents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_table('data/sentiment_label_sentences/________',  \n",
    "                   sep='\\t', \n",
    "                   header=None,\n",
    "                   names=['_____', '_____'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case\n",
    "df[_____] = df._____.map(lambda x: x.lower())  \n",
    "\n",
    "# remove punctuation\n",
    "df[_____] = df._____.str.replace('[^\\w\\s]', '')  \n",
    "\n",
    "# tokenize\n",
    "import nltk  \n",
    "nltk.download('punkt')\n",
    "df[_____] = df[_____].apply(nltk._____)\n",
    "\n",
    "# stem the words\n",
    "from nltk.stem import _____\n",
    "stemmer = _____()\n",
    "df[_____] = df[_____].apply(lambda x: [stemmer.stem(y) for y in x])  \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# This converts the list of words into space-separated strings and then switch\n",
    "# to term frequency format\n",
    "df[_____] = df[_____].apply(lambda x: ' '.join(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()  \n",
    "counts = count_vect.fit_transform(df[_____])  \n",
    "print(len(count_vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer().fit(counts)\n",
    "counts = transformer.transform(counts)  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(counts, df[_____], \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=69) \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB().fit(X_train, y_train)  \n",
    "import numpy as np\n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "print(np.mean(predicted == y_test)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
